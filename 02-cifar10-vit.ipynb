{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af22aebc",
   "metadata": {},
   "source": [
    "## (실습)  Vision Transformer Fine-Tuning - cifar10\n",
    "- 목표\n",
    " 1. ViT code 이해\n",
    " 2. pretrain 모델(ViT-b16) 이용 imagenet 1000 classes classification 실습\n",
    " 3. pretrain 모델(ViT-b16) 이용 small dataset(cifar10)에서 fine-tuning 실습\n",
    "\n",
    "- References \n",
    "  1. [vit-keras](https://github.com/faustomorales/vit-keras)\n",
    "  1. [pretrained models](https://github.com/faustomorales/vit-keras/releases/)\n",
    "  1. [vit-cifar10 code](https://github.com/kentaroy47/vision-transformers-cifar10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad84d32",
   "metadata": {},
   "source": [
    "Use vit-keras package\n",
    "\n",
    "## Usage\n",
    "Install vit-keras package using \n",
    "`pip install vit-keras --user`\n",
    "\n",
    "\n",
    "[//]: # (`pip install vit-keras tensorflow-addons tensorflow-addons --user`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d21e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try install in terminal \n",
    "# !pip3 install vit-keras --user\n",
    "# !pip3 install tensorflow-addons --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4890839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, BatchNormalization, Flatten, Dropout, Activation, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.datasets import cifar10\n",
    "from keras import initializers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from vit_keras import vit, utils, visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf51b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to download models from huggingface, it is necessary to set the following proxy and ssl \n",
    "# 수원 mlp proxy 및 ssl 설정\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# suwon\n",
    "import os\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "os.environ['HTTP_PROXY'] ='http://75.17.107.42:8080'\n",
    "os.environ['HTTPS_PROXY'] ='http://75.17.107.42:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f80157",
   "metadata": {},
   "source": [
    "## 1. ImageNet 1000 Classes Classification \n",
    "\n",
    "### Downloading pre-trained Weights\n",
    "\n",
    "- ViT-B16 \n",
    " * patch size = 16\n",
    " * input image_size = 224\n",
    " * Transformer-encoder configurations \n",
    "| |L(num_layers)| H(heads) | D(hidden size)| d_mlp (MLP Size) | params |\n",
    "|--|--|--|--|--|--|\n",
    "| Base |12|12|768|3072| 86M |\n",
    "| Large |24|16|1024|4096| 307M |\n",
    "\n",
    "- Classification Head\n",
    "  * Representation Layer : 본 실험에서는 사용 안함. imagenet21k weight 사용시 로드됨\n",
    "                           (vit-keras code 참조)\n",
    "- dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_size = 244 if b16 or b32 and 384 if l16 or l32\n",
    "image_size = 224 # or 384\n",
    "\n",
    "classes = utils.get_imagenet_classes()\n",
    "model = vit.vit_b16(\n",
    "    image_size=image_size,\n",
    "    activation='softmax', \n",
    "    pretrained=True,\n",
    "    include_top=True,\n",
    "    pretrained_top=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58ecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "keras.utils.plot_model(model, to_file=\"vit-b16-in1k.png\", show_shapes=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./image 내 파일을 보고 싶으면, 아래줄 uncomment 해서 실행해서 test image를 살펴보세요\n",
    "# !ls ./images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blocked in MLP에서는 외부 site 일부는 block되어 접근이 되지 않습니다.\n",
    "# 아래와 같이 바로 접근되는 site에 대해서 image 로드하여 classification 실습을 해 볼 수 있습니다.\n",
    "# url = 'https://d1bg8rd1h4dvdb.cloudfront.net/img/storypick/monamipet/2019/01/1811_pet_dog_pomeranian_m_01.jpg'\n",
    "\n",
    "# local storage에 저장된 file로 먼저 실습을 진행해 봅시다.\n",
    "image_path = './images/'\n",
    "url1 = image_path + 'Granny_smith_and_cross_section.jpg'\n",
    "url2 = image_path + 'Free!_(3987584939).jpg'\n",
    "url = url1\n",
    "\n",
    "image = utils.read(url, image_size)\n",
    "X = vit.preprocess_inputs(image).reshape(1, image_size, image_size, 3)\n",
    "y = model.predict(X)\n",
    "print(f'imagenet_class[{y[0].argmax()}] = ',classes[y[0].argmax()]) # Granny smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(image); plt.axis('off')  # uncomment if you want to see the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d683f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometime does not work (check storage or network connections)\n",
    "\n",
    "# imagenet validation dataset\n",
    "# https://github.com/ndb796/Small-ImageNet-Validation-Dataset-1000-Classes/tree/main/ILSVRC2012_img_val_subset\n",
    "\n",
    "#url = 'https://github.com/ndb796/Small-ImageNet-Validation-Dataset-1000-Classes/blob/main/ILSVRC2012_img_val_subset/0/ILSVRC2012_val_00000293.JPEG?raw=true'\n",
    "url = 'https://github.com/ndb796/Small-ImageNet-Validation-Dataset-1000-Classes/blob/main/ILSVRC2012_img_val_subset/100/ILSVRC2012_val_00000466.JPEG?raw=true'\n",
    "image = utils.read(url, image_size)\n",
    "X = vit.preprocess_inputs(image).reshape(1, image_size, image_size, 3)\n",
    "y = model.predict(X)\n",
    "print(f'imagenet_class[{y[0].argmax()}] = ',classes[y[0].argmax()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9781811",
   "metadata": {},
   "source": [
    "## Visualize attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_map = visualize.attention_map(model=model, image=image)\n",
    "print('Prediction:', classes[\n",
    "    model.predict(vit.preprocess_inputs(image)[np.newaxis])[0].argmax()]\n",
    ")  # Prediction: Eskimo dog, husky\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "ax1.axis('off')\n",
    "ax2.axis('off')\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(image)\n",
    "_ = ax2.imshow(attention_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c5643c",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning using a ViT-B16 model \n",
    "\n",
    "### ViT-B16 \n",
    "  - patch_size = 16, image_size = 224\n",
    "  - paramters: 81MB (85,844,736 B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2024\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a584d7",
   "metadata": {},
   "source": [
    "## Load Cifar10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_data, y_train_label), (x_test_data, y_test_label) = cifar10.load_data()\n",
    "x_train_data = (x_train_data/255.).astype(\"float32\")\n",
    "x_test_data = (x_test_data/255.).astype(\"float32\")\n",
    "y_train_label = to_categorical(y_train_label)\n",
    "y_test_label = to_categorical(y_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67705a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22422bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_data1, y_train_label1), (x_test_data1, y_test_label) = cifar10.load_data()\n",
    "x_test_data = (x_test_data1/255.).astype(\"float32\")\n",
    "y_test_label = to_categorical(y_test_label)\n",
    "x_test = x_test_data\n",
    "y_test = y_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd5cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset을 train(75%, 37,500)과 valid set(25%, 12,500)으로 나누기 (default=75%:25%)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_data, y_train_label, \n",
    "                                                      random_state=seed, shuffle=True)\n",
    "# train:valid:test = 62.5%: 20.8%: 16.7%= 37,500:12,500: 10,000 (images)\n",
    "x_test = x_test_data\n",
    "y_test = y_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d5f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_data.shape,x_test_data.shape, x_train.shape, x_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[0]/x_train_data.shape[0] # = 375/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25245c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test ratio\n",
    "dataset_ratio = np.array([x_train.shape[0], x_valid.shape[0], x_test.shape[0]])/(x_train_data.shape[0]+x_test.shape[0])\n",
    "# np.array([37500, 12500, 10000])/60000\n",
    "print(f'train:val:test={dataset_ratio[0]:.3f}:{dataset_ratio[1]:.3f}:{dataset_ratio[2]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1d991c",
   "metadata": {},
   "source": [
    "### Makes data augmentation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.2, zoom_range=0.2, \n",
    "                             horizontal_flip=True)\n",
    "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded271e2",
   "metadata": {},
   "source": [
    "### Modeling using ViT\n",
    "\n",
    "**NOTE** \n",
    "- Resize input image: 32x32x3 --> 224x224x3\n",
    "- Resize using tf `tf.image.resize`\n",
    "- Fine-tuning : 2 steps (5m + 40~50m, 98%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ca5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3) # Cifar10 image size\n",
    "image_size = 224          # size after resizing image\n",
    "num_classes = 10\n",
    "model_type='vit_b16'\n",
    "\n",
    "def build_model():\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = keras.layers.Lambda(\n",
    "            lambda image: tf.image.resize(image, (image_size, image_size))\n",
    "            )(inputs) # Resize image to  size 224x224\n",
    "    \n",
    "    base_model = vit.vit_b16(image_size=image_size, pretrained=True,\n",
    "                             include_top=False, pretrained_top=False, \n",
    "                             activation=\"softmax\" # not used if include_top=False\n",
    "                            )\n",
    "    base_model.trainable = False # Set false for transfer learning\n",
    "    \n",
    "    x = base_model(x) # x(output): 768\n",
    "    x = Dense(32, activation=keras.activations.gelu)(x)\n",
    "    x = Activation(keras.activations.gelu, name='feature')(x) # feature vector layer 이름 남김\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    model_final = Model(inputs=inputs, outputs=outputs)\n",
    "    return model_final, base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c93fb2",
   "metadata": {},
   "source": [
    "#### Step1: Train only top layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4892958",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, base_model = build_model()\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), \n",
    "              loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c650027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba422a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[0]/batch_size # 37500/32 == 1171.8 steps/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903d981",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "history1 = model.fit(train_generator,\n",
    "          steps_per_epoch=200, # you can delete this parameter to achieve more accuracy, \n",
    "                               # but takes much time.\n",
    "          epochs=2, \n",
    "          validation_data=(x_valid, y_valid),\n",
    "         )\n",
    "end_time = time.time()\n",
    "print(f\"training time = {end_time-start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4504311a",
   "metadata": {},
   "source": [
    "#### Step 2:Train entire layers again (Fine tuning step)\n",
    "- Info \n",
    "  * training time: 2-3 min/2 epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380e272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set training callbacks\n",
    "# ReduceLROnPlateau: \n",
    "# reduce learning rate by a factor of 2-10 once learning stagnates\n",
    "# monitor: 기준이 되는 값, val_loss가 더이상 감소되지 않을 경우\n",
    "# factor: learning rate 감소량, learning_rate *= factor \n",
    "# patient: Training이 진행됨에도 더이상 monitor되는 값의 개선이 없을 경우, \n",
    "#          최적의 monitor 값을 기준으로 몇 번의 epoch을 진행하고, \n",
    "#          learning rate를 조절할 지의 값\n",
    "# verbose: 1(Early stopping 적용시 화명에 적용 메시지 나타남) or 0(화면 메시지 없이 종료)\n",
    "# mode : \"auto\", \"min\"(loss), \"max\"(accuracy)\n",
    "#      : monitor가 최소가 좋은지 최대가 좋은지 \n",
    "plateau = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, patience=1, verbose=1)\n",
    "\n",
    "# EarlyStopping\n",
    "# Stop training when a monitored metric has stopped improving.\n",
    "# A model.fit() training loop will check at end of every epoch \n",
    "# whether the loss is no longer decreasing,\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1)\n",
    "\n",
    "# Switch ViT layer to trainable for fine tuning\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d725ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires compile again to activate trainable=True\n",
    "model.compile(optimizer=optimizers.SGD(learning_rate=0.001, momentum=0.9), \n",
    "              loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "start_time = time.time()\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=200, #you can delete this parameter to achieve more accuracy, \n",
    "                                         # but takes much time.\n",
    "                    epochs=10,\n",
    "                    validation_data=(x_valid, y_valid),\n",
    "                    callbacks=[plateau, earlystopping]\n",
    "                   )\n",
    "end_time = time.time()\n",
    "print(f\"training time = {end_time-start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b2505",
   "metadata": {},
   "source": [
    "## Plot the train history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65da606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history): \n",
    "    fig, ax = plt.subplots(figsize=(20, 9), nrows=1, ncols=2)\n",
    "    ax[0].plot(history.history[\"loss\"], c=\"r\", label=\"train loss\")\n",
    "    ax[0].plot(history.history[\"val_loss\"], c=\"b\", label=\"val loss\")\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(history.history[\"accuracy\"], c=\"r\", label=\"train accuracy\")\n",
    "    ax[1].plot(history.history[\"val_accuracy\"], c=\"b\", label=\"val accuracy\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_ylabel(\"Accuracy\")\n",
    "    ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b74b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61266e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning accuracy/time: 40~60 min/10 epoch \n",
    "# vit-b16: test accuracy - 98.1~4%, \n",
    "# sgd(0.01,0.9)-sgd(0.001,0.9), 98.1%, 3476 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6d9cc",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35354b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(\"\\nTest Accuracy: \", accuracy_score(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a8c8a8",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test dataset.\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# For each sample image in the test dataset, select the class label with the highest probability.\n",
    "predicted_labels = [np.argmax(i) for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5106ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded labels to integers.\n",
    "y_test_integer_labels = tf.argmax(y_test, axis=1)\n",
    "\n",
    "# Generate a confusion matrix for the test dataset.\n",
    "cm = tf.math.confusion_matrix(labels=y_test_integer_labels, predictions=predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap.\n",
    "plt.figure(figsize=[12, 6])\n",
    "import seaborn as sn\n",
    "sn.heatmap(cm, annot=True, fmt='d', annot_kws={\"size\": 12})\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7389cd00",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_dataset, y_label, model):\n",
    "\n",
    "    class_names = ['airplane',\n",
    "                   'automobile',\n",
    "                   'bird',\n",
    "                   'cat',\n",
    "                   'deer',\n",
    "                   'dog',\n",
    "                   'frog',\n",
    "                   'horse',\n",
    "                   'ship',\n",
    "                   'truck' ]\n",
    "    num_rows = 3\n",
    "    num_cols = 6\n",
    "    \n",
    "    # Retrieve a number of images from the dataset.\n",
    "    data_batch = x_dataset[0:num_rows*num_cols]\n",
    "\n",
    "    # Get predictions from model.  \n",
    "    predictions = model.predict(data_batch)\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    num_matches = 0\n",
    "        \n",
    "    for idx in range(num_rows*num_cols):\n",
    "        ax = plt.subplot(num_rows, num_cols, idx + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(data_batch[idx])\n",
    "\n",
    "        pred_idx = tf.argmax(predictions[idx]).numpy()\n",
    "        truth_idx = np.nonzero(y_label[idx])\n",
    "            \n",
    "        title = str(class_names[truth_idx[0][0]]) + \" : \" + str(class_names[pred_idx])\n",
    "        title_obj = plt.title(title, fontdict={'fontsize':13})\n",
    "            \n",
    "        if pred_idx == truth_idx:\n",
    "            num_matches += 1\n",
    "            plt.setp(title_obj, color='g')\n",
    "        else:\n",
    "            plt.setp(title_obj, color='r')\n",
    "                \n",
    "        acc = num_matches/(idx+1)\n",
    "    print(\"Prediction accuracy: \", int(100*acc)/100)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f821ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92015f2",
   "metadata": {},
   "source": [
    "## Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff467cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_categories=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb674389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run inference on new data\n",
    "# save an image from cifar10 dataset and load it\n",
    "image_id = 0 # cifar10 test image id in number\n",
    "image_file = f'./cifar10_test{image_id}.png'\n",
    "keras.utils.save_img(image_file, x_test[image_id])\n",
    "img = keras.utils.load_img(image_file, target_size=(32,32))\n",
    "\n",
    "# image display\n",
    "plt.imshow(img)\n",
    "img_array = keras.utils.img_to_array(img)\n",
    "\n",
    "# add a dimension to make an array ( a form of a list of images) for model.predict input\n",
    "# [image1]\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# preprocessing the input image  \n",
    "img_array = img_array.astype('float32') / 255\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "#score = predictions[0]\n",
    "\n",
    "prediction = np.argmax(predictions[0]) # predicted class id : id for max scores\n",
    "\n",
    "# categories[prediction]\n",
    "print(f\"ground truth_label = {cifar10_categories[np.argmax(y_test[image_id])]}, predicted_label = {cifar10_categories[prediction]}\")\n",
    "print(f\"Similarity = {np.dot(predictions[0],y_test[image_id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ab5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "model_file = 'cifar10_%s' % model_type\n",
    "# Using the save() method, the model will be saved to the file system in the 'tf SavedModel' format \n",
    "# or .'h5'.\n",
    "model.save(model_file) # tf format\n",
    "# model.save('%s.h5' % model_file) # h5 : does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {model_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f8c86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the saved model whether it can be loaded \n",
    "reloaded_model = keras.models.load_model(model_file)\n",
    "reloaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96dee59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
