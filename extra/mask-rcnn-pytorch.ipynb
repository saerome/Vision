{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50446bc1",
   "metadata": {},
   "source": [
    "# MASK R-CNN using pytorch, torchvision\n",
    "\n",
    "- object detection, instance segmentation\n",
    "- Ref: https://github.com/spmallick/learnopencv/blob/master/PyTorch-Mask-RCNN/PyTorch_Mask_RCNN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e835d",
   "metadata": {},
   "source": [
    "Install cv2 in venv_torch2\n",
    "\n",
    "```bash\n",
    "source ~/Workspace/venv_torch2/bin/activate\n",
    "pip3 install opencv-python\n",
    "pip3 freeze > ~/Workpace/venv_torch2/requirements\n",
    "deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a098059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "# These are the classes that are available in the COCO-Dataset\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "# Regarding N/A (10 missing categories) refer to https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "# 80 categories are released in 2014 and 2017 and used\n",
    "# FYI, the original paper mentioned 91 categories (10 N/A's, __background__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece0792",
   "metadata": {},
   "source": [
    "## Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the pretrained model from torchvision.models\n",
    "# Note: pretrained=True will get the pretrained weights for the model.\n",
    "# model.eval() mode change to use the model for inference (from train mode)\n",
    "#model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=True) # deprecated, userwarning\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='MaskRCNN_ResNet50_FPN_Weights.DEFAULT')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_colour_masks(image):\n",
    "    \"\"\"\n",
    "    random_colour_masks\n",
    "    parameters:\n",
    "      - image - predicted masks\n",
    "    method:\n",
    "      - the masks of each predicted object is given random colour for visualization\n",
    "    \"\"\"\n",
    "    colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    r[image == 1], g[image == 1], b[image == 1] = colours[random.randrange(0,10)]\n",
    "    coloured_mask = np.stack([r, g, b], axis=2)\n",
    "    return coloured_mask\n",
    "\n",
    "def get_prediction(img_path, threshold):\n",
    "    \"\"\"\n",
    "    get_prediction\n",
    "    parameters:\n",
    "      - img_path - path of the input image\n",
    "    method:\n",
    "      - Image is obtained from the image path\n",
    "      - the image is converted to image tensor using PyTorch's Transforms\n",
    "      - image is passed through the model to get the predictions\n",
    "      - masks, classes and bounding boxes are obtained from the model and soft masks are made binary(0 or 1) on masks\n",
    "        ie: eg. segment of cat is made 1 and rest of the image is made 0\n",
    "\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path) # load an image from a file path \n",
    "    transform = T.Compose([T.ToTensor()]) # define transform to tensor\n",
    "    img = transform(img).to(device) # image to tensor image\n",
    "    pred = model([img])  # model prediction\n",
    "    pred_score = list(pred[0]['scores'].detach().cpu().numpy()) # scores :tensor to numpy, detach() is required to remove grad info \n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n",
    "    # tensor.squeeze(): remove dimension 1, # ex. [3, 1, 20, 128] -> [3, 20, 128]\n",
    "    masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy() # gpu to cpu, to numpy\n",
    "    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].detach().cpu().numpy())]\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().cpu().numpy())]\n",
    "    masks = masks[:pred_t+1]\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    pred_class = pred_class[:pred_t+1]\n",
    "    return masks, pred_boxes, pred_class\n",
    "\n",
    "def instance_segmentation(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n",
    "    \"\"\"\n",
    "    instance_segmentation\n",
    "    parameters:\n",
    "      - img_path - path to input image\n",
    "    method:\n",
    "      - prediction is obtained by get_prediction\n",
    "      - each mask is given random color\n",
    "      - each mask is added to the image in the ration 1:0.8 with opencv\n",
    "      - final output is displayed\n",
    "    \"\"\"\n",
    "    masks, boxes, pred_cls = get_prediction(img_path, threshold)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    for i in range(len(masks)):\n",
    "        rgb_mask = random_colour_masks(masks[i])\n",
    "        img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
    "        box_i1 = (int(boxes[i][0][0]), int(boxes[i][0][1]))\n",
    "        box_i2 = (int(boxes[i][1][0]), int(boxes[i][1][1]))\n",
    "        cv2.rectangle(img, box_i1, box_i2,color=(0, 255, 0), thickness=rect_th)\n",
    "        cv2.putText(img,pred_cls[i], box_i1, cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
    "    plt.figure(figsize=(20,30))\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the following colors to fill the pixels\n",
    "colours = [[0, 255, 0],\n",
    "           [0, 0, 255],\n",
    "           [255, 0, 0],\n",
    "           [0, 255, 255],\n",
    "           [255, 255, 0],\n",
    "           [255, 0, 255],\n",
    "           [80, 70, 180],\n",
    "           [250, 80, 190],\n",
    "           [245, 145, 50],\n",
    "           [70, 150, 250],\n",
    "           [50, 190, 190]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78b43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images in this path to be tested\n",
    "image_path = '../images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c0e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the image \n",
    "\n",
    "img = Image.open(image_path+'./mrcnn_person.jpg')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f'image size:{img.size}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference on the image\n",
    "# T.Compose: can compose of a series of preprocessing, for example, resize, random horizonal flip, totensor, normalize\n",
    "# here, tranform to tensor for model input\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "img_tensor = transform(img).to(device) \n",
    "# predict from input image tensor in array\n",
    "pred = model([img_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f91c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at what the `pred`, the result, looks like.\n",
    "# `pred` is a list of dictionaries, since we had passed a single image, we will get a single-item list\n",
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will keep only the pixels with values  greater than 0.5 as 1, and set the rest to 0.\n",
    "masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
    "masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06759e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the mask for the `person` class since the 0th mask belongs to `person`\n",
    "plt.imshow(masks, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabc737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's color the `person` mask using the `random_colour_masks` function\n",
    "mask1 = random_colour_masks(masks)\n",
    "plt.imshow(mask1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52122790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's blend the original and the masked image and plot it.\n",
    "blend_img = cv2.addWeighted(np.asarray(img), 0.5, mask1, 0.5, 0)\n",
    "\n",
    "plt.imshow(blend_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a998618",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation(image_path+'./mrcnn_standing_people.jpg', 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation(image_path+'./mrcnn_cars.jpg', 0.9, rect_th=5, text_size=5, text_th=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743bc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation(image_path+'./mrcnn_traffic.jpg', 0.6, rect_th=2, text_size=2, text_th=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation(image_path+'./mrcnn_birds.jpg', 0.9)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation(image_path+'./mrcnn_people.jpg', 0.8, rect_th=1, text_size=1, text_th=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b7996",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation(image_path+'./mrcnn_playing.jpg', 0.8, rect_th=6, text_size=6, text_th=6)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe35095",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation(image_path+'./mrcnn_cat_dog.jpg', 0.95, rect_th=5, text_size=5, text_th=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation(image_path+'./mrcnn_elephants.jpg', 0.9, rect_th=6, text_size=6, text_th=6)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fce6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation(image_path+'./mrcnn_baby_teddy.jpg', 0.95, rect_th=6, text_size=6, text_th=6)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc3214d",
   "metadata": {},
   "source": [
    "## Comparing the inference time of model in CPU & GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47bc0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inference_time(image_path, gpu=False):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "    img = Image.open(image_path)\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img = transform(img)\n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "        img = img.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "        img = img.cpu()\n",
    "    start_time = time.time()\n",
    "    pred = model([img])\n",
    "    end_time = time.time()\n",
    "    return end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inference_time(image_path, gpu=False):\n",
    "    #model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) # user warning\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='MaskRCNN_ResNet50_FPN_Weights.DEFAULT')\n",
    "    model.eval()\n",
    "    img = Image.open(image_path)\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    img = transform(img)\n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "        img = img.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "        img = img.cpu()\n",
    "    start_time = time.time()\n",
    "    pred = model([img])\n",
    "    end_time = time.time()\n",
    "    return end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run inference on all the downloaded images and average their inference time \n",
    "img_paths = [path for path in os.listdir(image_path) if (path.split(\".\")[-1].lower() in [\"jpeg\", \"jpg\"]) and path.startswith('mrcnn_') ]\n",
    "\n",
    "gpu_time = sum([check_inference_time(image_path+img_path, gpu=True) for img_path in img_paths])/len(img_paths)\n",
    "cpu_time = sum([check_inference_time(image_path+img_path, gpu=False) for img_path in img_paths])/len(img_paths)\n",
    "\n",
    "print('\\n\\nAverage Time take by the model with GPU = {}s\\nAverage Time take by the model with CPU = {}s'.format(gpu_time, cpu_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10facdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_time/gpu_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5734fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor comments\n",
    "# this category names include \"N/A\"\n",
    "\n",
    "# len(COCO_INSTANCE_CATEGORY_NAMES)\n",
    "# nas = [ c for c in COCO_INSTANCE_CATEGORY_NAMES if c == \"N/A\"]\n",
    "# len(nas), nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac6f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch2i",
   "language": "python",
   "name": "venv_torch2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
