{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149f9c43",
   "metadata": {},
   "source": [
    "## SAM(Segment Anything Model) 실습\n",
    "\n",
    "https://github.com/huggingface/notebooks/blob/main/examples/segment_anything.ipynb\n",
    "\n",
    "Reference: [original Segment-Anything github](https://github.com/facebookresearch/segment-anything/blob/main/notebooks/predictor_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420df51",
   "metadata": {},
   "source": [
    "- it was tested in MLP-suwon P40 [O]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b9760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to download models from huggingface, it is necessary to set the following proxy and ssl \n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# suwon\n",
    "import os\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "os.environ['HTTP_PROXY'] ='http://75.17.107.42:8080'\n",
    "os.environ['HTTPS_PROXY'] ='http://75.17.107.42:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfc8d4",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ad9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))  \n",
    "\n",
    "def show_boxes_on_image(raw_image, boxes):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(raw_image)\n",
    "    for box in boxes:\n",
    "        show_box(box, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "def show_points_on_image(raw_image, input_points, input_labels=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(raw_image)\n",
    "    input_points = np.array(input_points)\n",
    "    if input_labels is None:\n",
    "        labels = np.ones_like(input_points[:, 0])\n",
    "    else:\n",
    "        labels = np.array(input_labels)\n",
    "    show_points(input_points, labels, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "def show_points_and_boxes_on_image(raw_image, boxes, input_points, input_labels=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(raw_image)\n",
    "    input_points = np.array(input_points)\n",
    "    if input_labels is None:\n",
    "        labels = np.ones_like(input_points[:, 0])\n",
    "    else:\n",
    "        labels = np.array(input_labels)\n",
    "    show_points(input_points, labels, plt.gca())\n",
    "    for box in boxes:\n",
    "        show_box(box, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_points_and_boxes_on_image(raw_image, boxes, input_points, input_labels=None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(raw_image)\n",
    "    input_points = np.array(input_points)\n",
    "    if input_labels is None:\n",
    "        labels = np.ones_like(input_points[:, 0])\n",
    "    else:\n",
    "        labels = np.array(input_labels)\n",
    "    show_points(input_points, labels, plt.gca())\n",
    "    for box in boxes:\n",
    "        show_box(box, plt.gca())\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_masks_on_image(raw_image, masks, scores):\n",
    "    if len(masks.shape) == 4:\n",
    "        masks = masks.squeeze()\n",
    "    if scores.shape[0] == 1:\n",
    "        scores = scores.squeeze()\n",
    "\n",
    "    nb_predictions = scores.shape[-1]\n",
    "    fig, axes = plt.subplots(1, nb_predictions, figsize=(15, 15))\n",
    "\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        mask = mask.cpu().detach()\n",
    "        axes[i].imshow(np.array(raw_image))\n",
    "        show_mask(mask, axes[i])\n",
    "        axes[i].title.set_text(f\"Mask {i+1}, Score: {score.item():.3f}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7645382",
   "metadata": {},
   "source": [
    "### SAM processing flow\n",
    "(input(image, points, boxes, labels)) &rarr; SamProcessor &rarr; (inputs:dict + image_embeddings)   \n",
    "&rarr; SamModel &rarr; (output(masks))   \n",
    "&rarr; SamProcessor.post_process_masks &rarr; vizualization \n",
    "\n",
    "- image embedding은 한번 해서 반복적으로 사용\n",
    "  * image_embeddings = model(SamModel).get_image_embeddings(inputs[\"pixel_values\"])\n",
    "- 참조: [SamModel](https://huggingface.co/docs/transformers/v4.43.2/en/model_doc/sam#transformers.SamModel), \n",
    "  [SamImageProcessor](https://huggingface.co/docs/transformers/v4.43.2/en/model_doc/sam#transformers.SamImageProcessor)\n",
    "\n",
    "### Model loading\n",
    "\n",
    "- facebook의 sam-vit-huge pre-trained 모델 loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c69f73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea613e",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92ac37",
   "metadata": {},
   "source": [
    "### Load the example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# 최근 mlp에서 block\n",
    "#img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
    "#raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "dataset_path = '/group-volume/sr_edu/AI-Application-Specialist-Vision-Dataset/hf-assets/'\n",
    "img_file = dataset_path  + \"car.png\"\n",
    "raw_image = Image.open(img_file).convert(\"RGB\")\n",
    "\n",
    "plt.imshow(raw_image)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afbfc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f4852",
   "metadata": {},
   "source": [
    "#### Step 1: Retrieve the image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55614e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_tensors=\"pt\": return tensor.Tensor. (If \"tf\", returns tf.Tensor)\n",
    "# - https://huggingface.co/docs/transformers/model_doc/sam\n",
    "inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "image_embeddings = model.get_image_embeddings(inputs[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys(), inputs['pixel_values'].shape, inputs['reshaped_input_sizes'], image_embeddings.shape, 1024/2646*1764\n",
    "#  reshaped_input_size: masking 처리를 위한 (1024로 입력 영상 크기로 resize시) 1024x1024에서 원영상의 크기 정보\n",
    "#   예) original image size: 2646x1764\n",
    "#       모델 입력 image size: 1024x1024 (1024x683 영역이 original image의 영역, 나머지는 padding)\n",
    "#       683 = 1024/2646 * 1764"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c35bbf",
   "metadata": {},
   "source": [
    "## Usecase 1: Feed a set of 2D points to predict a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single point\n",
    "input_points = [[[450, 600]]] # batch x points x point(x, y)\n",
    "show_points_on_image(raw_image, input_points[0])\n",
    "#plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0784c",
   "metadata": {},
   "source": [
    "See image_processor.post_process_masks: [API manual](https://huggingface.co/docs/transformers/v4.43.2/en/model_doc/sam#transformers.SamImageProcessor.post_process_masks), \n",
    "[code](https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/models/sam/image_processing_sam.py#L631)\n",
    "- (pred_masks) &rarr; Bilinear Interpolation &rarr; (masks w/ reshaped_input_sizes)   \n",
    "  &rarr; Bilinear Interpolation &rarr; (masks w/ original image size)  \n",
    "  &rarr; Binarization &rarr; (binary masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(raw_image, input_points=input_points, \n",
    "                   return_tensors=\"pt\").to(device)\n",
    "# pop the pixel_values as they are not neded\n",
    "inputs.pop(\"pixel_values\", None)\n",
    "inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "                                    outputs.pred_masks.cpu(),  \n",
    "                                    inputs[\"original_sizes\"].cpu(), \n",
    "                                    inputs[\"reshaped_input_sizes\"].cpu())\n",
    "scores = outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a95be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8665f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks[0].shape, scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a7cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks_on_image(raw_image, masks[0], scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbde589",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1061b4c",
   "metadata": {},
   "source": [
    "### To predict a single mask using a set of points\n",
    "\n",
    "- 두개 point 사용한 1 mask prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c39b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = [[[550, 600], [2100, 1000]]]\n",
    "show_points_on_image(raw_image, input_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac519f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(raw_image, input_points=input_points, \n",
    "                   return_tensors=\"pt\").to(device)\n",
    "# pop the pixel_values as they are not neded\n",
    "inputs.pop(\"pixel_values\", None)\n",
    "inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "                                    outputs.pred_masks.cpu(), \n",
    "                                    inputs[\"original_sizes\"].cpu(), \n",
    "                                    inputs[\"reshaped_input_sizes\"].cpu())\n",
    "scores = outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3257b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks_on_image(raw_image, masks[0], scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb7705",
   "metadata": {},
   "source": [
    "## Usecase 2: Predict segmentations masks using bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a4ffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = [[[650, 900, 1000, 1250]]]\n",
    "\n",
    "show_boxes_on_image(raw_image, input_boxes[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(raw_image, input_boxes=[input_boxes], \n",
    "                   return_tensors=\"pt\").to(device)\n",
    "inputs[\"input_boxes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b042d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.pop(\"pixel_values\", None)\n",
    "inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "                                    outputs.pred_masks.cpu(), \n",
    "                                    inputs[\"original_sizes\"].cpu(), \n",
    "                                    inputs[\"reshaped_input_sizes\"].cpu())\n",
    "scores = outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3342ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks_on_image(raw_image, masks[0], scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60e424",
   "metadata": {},
   "source": [
    "## Usecase 3:Predict segmentation masks given points and bounding boxes\n",
    "\n",
    "- box하나, point 하나에 대한 mask predction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce17c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = [[[650, 900, 1000, 1250]]]\n",
    "input_points = [[[820, 1080]]]\n",
    "\n",
    "show_points_and_boxes_on_image(raw_image, input_boxes[0], input_points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(raw_image, input_boxes=[input_boxes], \n",
    "                   input_points=[input_points], return_tensors=\"pt\").to(device)\n",
    "\n",
    "inputs.pop(\"pixel_values\", None)\n",
    "inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "                                    outputs.pred_masks.cpu(), \n",
    "                                    inputs[\"original_sizes\"].cpu(), \n",
    "                                    inputs[\"reshaped_input_sizes\"].cpu())\n",
    "scores = outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870fec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks_on_image(raw_image, masks[0][0], scores[:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e26baf6",
   "metadata": {},
   "source": [
    "### point의 label 0(제외할 영역) 입력으로 제공하여 box 내에서 point 바깥 영역에 대한 semantic mask prediction\n",
    "\n",
    "- 참조 : 3 label types\n",
    "  * 1: the point is a point that contains the object of interest\n",
    "  * 0: the point is a point that does not contain the object of interest\n",
    "  * -1: the point corresponds to the background\n",
    "  * [참조 link](https://huggingface.co/docs/transformers/model_doc/sam#transformers.SamModel.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6005ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = [[[650, 900, 1000, 1255]]]\n",
    "input_points = [[[820, 1080]]]\n",
    "labels = [0]\n",
    "\n",
    "show_points_and_boxes_on_image(raw_image, input_boxes[0], input_points[0], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9875cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = [[[620, 900, 1000, 1255]]]\n",
    "input_points = [[[820, 1080]]]\n",
    "labels = [[0]]\n",
    "inputs = processor(raw_image, input_boxes=[input_boxes], \n",
    "                   input_points=[input_points], input_labels=[labels], \n",
    "                   return_tensors=\"pt\").to(device)\n",
    "\n",
    "inputs.pop(\"pixel_values\", None)\n",
    "inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "                                    outputs.pred_masks.cpu(), \n",
    "                                    inputs[\"original_sizes\"].cpu(), \n",
    "                                    inputs[\"reshaped_input_sizes\"].cpu())\n",
    "scores = outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad87b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks_on_image(raw_image, masks[0][0], scores[:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073116a7",
   "metadata": {},
   "source": [
    ": 0으로 label된 point 지정 영역을 제외한 외부의 영역에 대한 mask가 찾아짐을 볼 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e024a",
   "metadata": {},
   "source": [
    "## Usecase 4: Predict multiple masks per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5153be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = [[[850, 1100], [2250, 1000]]]\n",
    "show_points_on_image(raw_image, input_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e2e30",
   "metadata": {},
   "source": [
    "### Sub-usecase 1: one prediction per point\n",
    "\n",
    "- 두개 point 입력: 각각에 대한 mask prections(두개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2efa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_points = [[[[850, 1100]], [[2250, 1000]]]]\n",
    "inputs = processor(raw_image, input_points=input_points, \n",
    "                   return_tensors=\"pt\").to(device)\n",
    "inputs[\"input_points\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.pop(\"pixel_values\", None)\n",
    "inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "                                    outputs.pred_masks.cpu(), \n",
    "                                    inputs[\"original_sizes\"].cpu(), \n",
    "                                    inputs[\"reshaped_input_sizes\"].cpu())\n",
    "scores = outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a88ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape, len(masks), masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 point에 대한 predicted mask 및 score(3개)\n",
    "show_masks_on_image(raw_image, masks[0][0], scores[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150aafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 point에 대한 predicted mask 및 score(3개)\n",
    "show_masks_on_image(raw_image, masks[0][1], scores[:, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d545a8e",
   "metadata": {},
   "source": [
    "### Sub-usecase 2: Feed multiple bounding boxes to the same image\n",
    "\n",
    "- 두개 box 입력: 각각에 대한 mask prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = [[[650, 900, 1000, 1250], [2050, 800, 2400, 1150]]]\n",
    "\n",
    "show_boxes_on_image(raw_image, input_boxes[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af9309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_boxes = [[[650, 900, 1000, 1250], [2050, 800, 2400, 1150]]]\n",
    "inputs = processor(raw_image, input_boxes=input_boxes, \n",
    "                   return_tensors=\"pt\").to(device)\n",
    "inputs[\"input_boxes\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f44701",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.pop(\"pixel_values\", None)\n",
    "inputs.update({\"image_embeddings\": image_embeddings})\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, multimask_output=False)\n",
    "\n",
    "masks = processor.image_processor.post_process_masks(\n",
    "                                    outputs.pred_masks.cpu(), \n",
    "                                    inputs[\"original_sizes\"].cpu(), \n",
    "                                    inputs[\"reshaped_input_sizes\"].cpu())\n",
    "scores = outputs.iou_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.shape, len(masks), masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks_on_image(raw_image, masks[0], scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(ToDo) 3개 mask output을 다 보려면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069c97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddff136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_masks_on_image(raw_image, masks[0][0,:,:,:], scores[0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_masks_on_image(raw_image, masks[0][1,:,:,:], scores[0,1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374763ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch2i",
   "language": "python",
   "name": "venv_torch2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
