{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b61c33c4",
   "metadata": {},
   "source": [
    "# ResNet Modeling\n",
    "\n",
    "## Dataset: cifar 10\n",
    "## Model: ResNet v2\n",
    "- **ResNet v1** :  [Deep Residual Learning for Image Recognition, 2015](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "- **ResNet v2** : [Identity Mapping in Deep Residual Networks, 2016](https://arxiv.org/pdf/1603.05027)\n",
    "\n",
    "## Experiments\n",
    "- **ResNet v2** model로 실습, hyperparameter 변경 실습\n",
    "  * epoch \n",
    "  * ResNet block 수 n  \n",
    "  * subtract_pixel_mean = True/False 변경 \n",
    "- inference에서 test image 사용한 결과 보기\n",
    "\n",
    "If necessary, install scipy\n",
    "```{bash}\n",
    "!pip3 install scipy\n",
    "```\n",
    "\n",
    "\n",
    "Reference: https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter2-deep-networks/resnet-cifar10-2.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aac8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d91f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for display image and plot\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596bdaa",
   "metadata": {},
   "source": [
    "## 0. Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045965f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 # cifar10 classes : fixed\n",
    "data_augmentation = True\n",
    "\n",
    "# Subtracting pixel mean can improve accuracy\n",
    "subtract_pixel_mean = True\n",
    "\n",
    "batch_size = 32  # 128  # original paper trained all networks with batch_size=128\n",
    "epochs = 120     # 100, 120(~55m, 91%), 150, 200(1:30m), 300(2:30m, 91%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3509f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model version\n",
    "version = 2      # fixed\n",
    "\n",
    "# v1 n : number of residual blocks                        \n",
    "#     { 3,  5,  7,  9, ...}. cf. training time\n",
    "# v1 depth: the corresponding number of layers(depth): \n",
    "#           {20, 32, 44, 56, ... }\n",
    "# v2 n : number of residual blocks                        \n",
    "#     { 2,  3,  4,  5,  6,  7,  9, ...}. cf. training time\n",
    "# v2 depth: the corresponding number of layers(depth): \n",
    "#     {20, 29, 38, 47, 56, 65, 83, ... }\n",
    "\n",
    "n = 2 # 변경 가능하지만, 증가할 수록 학습 시간이 오래 걸립니다.\n",
    "# Computed depth from supplied model parameter n\n",
    "# 1 input Conv, last dense layer : 2 extra layers\n",
    "if version == 1:\n",
    "    # 3 stages x 2 (conv layers / ResNetBlock)  x n ((number of ResNetBlocks)/stage)\n",
    "    depth = n * 6 + 2 \n",
    "elif version == 2:\n",
    "    # 3 stage x 3 (conv layers/ResNetBlock) x n ((number of ResNetBlocks)/stage)\n",
    "    depth = n * 9 + 2 \n",
    "    \n",
    "print(f'model: ResNet-v{version}-{depth}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b45501",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "- Load cifar10 dataset\n",
    "- Normalize input(x) data\n",
    "- Output encoding to one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name, depth and version\n",
    "model_type = 'ResNet%dv%d' % (depth, version)\n",
    "\n",
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110580a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abcd878",
   "metadata": {},
   "source": [
    "## 2. Modeling: ResNet\n",
    "- Using functional APIs: Add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03a653",
   "metadata": {},
   "source": [
    "### Residual block F(x)\n",
    "- H(x) = F(x) + x\n",
    "- F(x) = f( resnet_layer, x)\n",
    " * F(x) : 2 resnet_layers  \n",
    "- f(x) = 2d convolution - Batch Normalization - Activation(relu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True: v1) or\n",
    "            bn-activation-conv (False: v2)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first: # v1\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else: # v2\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24e159",
   "metadata": {},
   "source": [
    "### Residual block F(x)\n",
    "1) idenity shortcut: H(x) = F(x) + x\n",
    "- F(x) = f( resnet_layer, x)\n",
    " * F(x) : 2 resnet_layers  \n",
    "        \n",
    "2) projection shortcut: H(x) = F(x) + Conv2d(x, stride=2)\n",
    "  - subsampling: Conv2d(stride=2)\n",
    "  - resnet_layer = Conv2d - BN - Activation(relu) \n",
    "  - resolution을 1/2로 줄일 때, stride=2(filter수는 두배씩 증가시킴) \n",
    "  - conv2d(3x3), filter size = 64 (at 32 x 32)\n",
    "\n",
    "## Modeling: ResNet v1\n",
    "- input block(first conv.) : Conv+BN+ReLU\n",
    "- middle blocks: 3 stages ResNet blocks (32x32 - 16x16 - 8x8)\n",
    "  * stage 0(32x32): 16 channels(no. of filters), n ResNet Blocks\n",
    "  * stage 1(16x16): 16x2 channels, n ResNet Blocks\n",
    "  * stage 2(8x8): 16x4 channels, n ResNet Blocks\n",
    "- output block(last classification layer): average pooling 2d (8)- flatten - dense(softmax, 10 classes)\n",
    "- depth = 2 + 6*n: number of weighted layers\n",
    "  * n: ResBlocks/stage\n",
    "  * input block(conv) + last dense layer : 2\n",
    "  * middle blocks: 6*n = 3 stages x 2 (conv layers/ResNet Block) * n (ResBlocks/stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c216571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "    Features maps sizes:\n",
    "    stage 0: 32x32, 16\n",
    "    stage 1: 16x16, 32\n",
    "    stage 2:  8x8,  64\n",
    "    The Number of parameters is approx the same as Table 6 of [a]:\n",
    "    ResNet20 0.27M\n",
    "    ResNet32 0.46M\n",
    "    ResNet44 0.66M\n",
    "    ResNet56 0.85M\n",
    "    ResNet110 1.7M\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
    "    # Start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='glorot_uniform')(y) \n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59202a2",
   "metadata": {},
   "source": [
    "### Residual block F(x)\n",
    "1) idenity mapping : H(x) = F(x) + x\n",
    " - F(x) = f( resnet_layer, x)\n",
    " - resnet_layer =  **BN** - Act(relu) - Conv2d - BN - Act(relu) - Conv2d - BN - Act(relu) - Conv2d \n",
    "   * **pre-activation** for identity mapping\n",
    "   * Activation: relu\n",
    " - F(x) : 3 conv's \n",
    " \n",
    "2) projection shortcut: H(x) = F(x) + Conv2d(x, stride=2)\n",
    "  - resolution 또는 channel 변경시 사용\n",
    "  - subsampling: Conv2d(stride=2)\n",
    "  - resolution을  1/2로 줄일 때 stride=2(filter수 는 두배로 증가)\n",
    "  - conv2d(3x3), filter size = 64(at 32x32) \n",
    "\n",
    "### Modeling: ResNet v2\n",
    "- Pre-activation 방식 ResNet Block 사용\n",
    "- input block: Conv+BN+ReLU\n",
    " * Conv 한번 수행\n",
    "- middle blocks: 3 stages로 구성. ResNet blocks (32x32 - 16x16 - 8x8)\n",
    " * stage 0(32x32): 16 channels(no. of filters), 64 channels for input/output, n ResNet Blocks\n",
    "   - Note the difference of the first block\n",
    " * stage 1(16x16): 64 channels, 128 channls for input/output, n ResNet Blocks\n",
    " * stage 2(8x8): 128 channels, 256 channels for input/output, n ResNet Blocks\n",
    "- output block (last classification layer): \n",
    " * BN-Act-average pooling 2d (256)- flatten - dense(softmax, 10 classes)\n",
    "- layer 수(depth) 계산: \n",
    "  depth = 2 + 9*n (n: ResBlocks/stage)\n",
    " * number of weighted layers in the middle blocks = 3 convs/resnet block * n resnet block/stage * 3 stages \n",
    " * input block(conv) + last dense layer : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or \n",
    "    also known as bottleneck layer.\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, \n",
    "    the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, \n",
    "    while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have \n",
    "    the same number filters and the same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    Arguments:\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    Returns:\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 110 in [b])')\n",
    "    # start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU\n",
    "    # on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                # first layer and first stage\n",
    "                if res_block == 0:  \n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                # first layer but not first stage\n",
    "                if res_block == 0:\n",
    "                    # downsample\n",
    "                    strides = 2 \n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection\n",
    "                # to match changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten(name='feature')(x) # feature vector layer 이름 남김\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='glorot_uniform')(y) \n",
    "\n",
    "    # instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc9da4",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "### - 3.1 **Optimizer**: **Adam** + lr scheduler\n",
    "#### Learning scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    \n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739ca08",
   "metadata": {},
   "source": [
    "- **Optimizer**: **Adam** + lr scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac233eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else: \n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr_schedule(0)),\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "print(model_type)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "callbacks = [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3096af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arch_png = 'model_cifar10_%s.png' % model_type\n",
    "keras.utils.plot_model(model, to_file=model_arch_png, show_shapes=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9e93c",
   "metadata": {},
   "source": [
    "### 3.2 Train the designed model\n",
    "- Data Augmentation\n",
    "- Traing time: ~1:30min/200 epoch, 91%(test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abcedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training, with or without data augmentation.\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    start_time = datetime.now()    \n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=False, #  True if necessary \n",
    "              callbacks=callbacks)\n",
    "    end_time = datetime.now()\n",
    "\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # this will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False)\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "    \n",
    "    steps_per_epoch =  math.ceil(len(x_train) / batch_size)\n",
    "    # fit the model on the batches generated by datagen.flow().\n",
    "    start_time = datetime.now()    \n",
    "    history = model.fit(x=datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "              verbose=1,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              callbacks=callbacks)\n",
    "    end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f'Training time:{elapsed_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e71a7",
   "metadata": {},
   "source": [
    "## Plot the train history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e62f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history.\n",
    "def plot_history(h):\n",
    "    epochs_ = len(h.history['loss'])\n",
    "    ep = np.arange(epochs_)\n",
    "    fig = plt.figure(figsize=(17,8))\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "\n",
    "    ax1.plot(ep, h.history['loss'], label='loss')\n",
    "    ax1.plot(ep, h.history['val_loss'], label='val_loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss ')\n",
    "    ax1.legend(bbox_to_anchor=(1,1))\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(ep, h.history['accuracy'], label='accuracy')\n",
    "    ax2.plot(ep, h.history['val_accuracy'], label='val_accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend(bbox_to_anchor=(1,1))\n",
    "    ax2.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19b7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6873c",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation\n",
    "- Accuracy on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee9b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39c607c",
   "metadata": {},
   "source": [
    "### Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80426a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test dataset.\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# For each sample image in the test dataset, select the class label with the highest probability.\n",
    "predicted_labels = [np.argmax(i) for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cccaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoded labels to integers.\n",
    "y_test_integer_labels = tf.argmax(y_test, axis=1)\n",
    "\n",
    "# Generate a confusion matrix for the test dataset.\n",
    "cm = tf.math.confusion_matrix(labels=y_test_integer_labels, predictions=predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap.\n",
    "plt.figure(figsize=[12, 6])\n",
    "import seaborn as sn\n",
    "sn.heatmap(cm, annot=True, fmt='d', annot_kws={\"size\": 12})\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a43a81",
   "metadata": {},
   "source": [
    "## Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_categories=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbd7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run inference on new data\n",
    "# save an image from cifar10 dataset and load it\n",
    "image_id = 0 # cifar10 test image id in number\n",
    "image_file = f'./cifar10_test{image_id}.png'\n",
    "keras.utils.save_img(image_file, x_test[image_id])\n",
    "img = keras.utils.load_img(image_file, target_size=(32,32))\n",
    "\n",
    "# image display\n",
    "plt.imshow(img)\n",
    "img_array = keras.utils.img_to_array(img)\n",
    "\n",
    "# add a dimension to make an array ( a form of a list of images) for model.predict input\n",
    "# [image1]\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# preprocessing the input image  \n",
    "img_array = img_array.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    img_array -= x_train_mean  # x_train_mean = np.mean(x_train, axis=0)\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "#score = predictions[0]\n",
    "\n",
    "prediction = np.argmax(predictions[0]) # predicted class id : id for max scores\n",
    "\n",
    "# categories[prediction]\n",
    "print(f\"ground truth_label = {cifar10_categories[np.argmax(y_test[image_id])]}, predicted_label = {cifar10_categories[prediction]}\")\n",
    "print(f\"Similarity = {np.dot(predictions[0],y_test[image_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387b83b",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f4b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(x_dataset, y_label, model):\n",
    "\n",
    "    class_names = ['airplane',\n",
    "                   'automobile',\n",
    "                   'bird',\n",
    "                   'cat',\n",
    "                   'deer',\n",
    "                   'dog',\n",
    "                   'frog',\n",
    "                   'horse',\n",
    "                   'ship',\n",
    "                   'truck' ]\n",
    "    num_rows = 3\n",
    "    num_cols = 6\n",
    "    \n",
    "    # Retrieve a number of images from the dataset.\n",
    "    data_batch = x_dataset[0:num_rows*num_cols]\n",
    "\n",
    "    # Get predictions from model.  \n",
    "    predictions = model.predict(data_batch)\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    num_matches = 0\n",
    "        \n",
    "    if subtract_pixel_mean:\n",
    "        data_batch += x_train_mean  # add to range [0,1] for display , x_train_mean = np.mean(x_train, axis=0)\n",
    "\n",
    "    for idx in range(num_rows*num_cols):\n",
    "        ax = plt.subplot(num_rows, num_cols, idx + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(data_batch[idx])\n",
    "\n",
    "        pred_idx = tf.argmax(predictions[idx]).numpy()\n",
    "        truth_idx = np.nonzero(y_label[idx])\n",
    "            \n",
    "        title = str(class_names[truth_idx[0][0]]) + \" : \" + str(class_names[pred_idx])\n",
    "        title_obj = plt.title(title, fontdict={'fontsize':13})\n",
    "            \n",
    "        if pred_idx == truth_idx:\n",
    "            num_matches += 1\n",
    "            plt.setp(title_obj, color='g')\n",
    "        else:\n",
    "            plt.setp(title_obj, color='r')\n",
    "                \n",
    "        acc = num_matches/(idx+1)\n",
    "    print(\"Prediction accuracy: \", int(100*acc)/100)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(x_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23737361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to load the model later\n",
    "model_file = 'cifar10_%s' % model_type\n",
    "model.save(model_file) # tf format\n",
    "#model.save('%s.h5' % model_file) # h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a55aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the saved model can be loaded \n",
    "reloaded_model = keras.models.load_model(model_file)\n",
    "reloaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f3023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
