{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3065e37c",
   "metadata": {},
   "source": [
    "## CLIP (Contrastive Language-Image Pre-Training) \n",
    "\n",
    "Reference:\n",
    "- https://huggingface.co/docs/transformers/model_doc/clip\n",
    "- https://github.com/mlfoundations/open_clip/blob/main/docs/Interacting_with_open_clip.ipynb\n",
    "- https://github.com/openai/CLIP\n",
    "\n",
    "Prerequisite\n",
    "- Install scikit-image \n",
    "```\n",
    "pip install scikit-image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85b67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb74047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to download models from huggingface, it is necessary to set the following proxy and ssl \n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# suwon\n",
    "import os\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'\n",
    "os.environ['HTTP_PROXY'] ='http://75.17.107.42:8080'\n",
    "os.environ['HTTPS_PROXY'] ='http://75.17.107.42:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/group-volume/sr_edu/AI-Application-Specialist-Vision-Dataset/'\n",
    "hf_path = dataset_path + 'hf-models/'\n",
    "clip_ckpt = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "local_model = hf_path + clip_ckpt\n",
    "model = CLIPModel.from_pretrained(local_model) \n",
    "processor = CLIPProcessor.from_pretrained(local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface에서 직접 download시\n",
    "#clip_ckpt = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "#model = CLIPModel.from_pretrained(clip_ckpt) \n",
    "#processor = CLIPProcessor.from_pretrained(clip_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c7b74",
   "metadata": {},
   "source": [
    "## Zero-shot Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001ad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as DisplayImage\n",
    "# from openai/clip: https://github.com/openai/CLIP?tab=readme-ov-file\n",
    "DisplayImage(dataset_path + 'hf-assets/clip-zero-shot-2.png', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dbb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ls ./images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be23b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image = Image.open(\"./images/cat.jpg\")\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a31b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"a photo of a cat\", \"a photo of a robot\", \"a photo of an apple\", \"a photo of cats\"]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "probs, texts[probs.argmax().cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d24cb",
   "metadata": {},
   "source": [
    "### 더 정확한 text 또는 다른 유사한 text 표현을 더 추가해 실습해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd872c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts = [\"a photo of a cat\", \"a photo of a robot\", \"a photo of a turtle\", \"a photo of a cat and a turtle\"]\n",
    "texts = [\"a photo of a cat\", \"a photo of a robot\", \"a photo of a turtle\", \"a photo of two cats\", \"a photo of three cats\"]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "probs, texts[probs.argmax().cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0daa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.config\n",
    "#outputs\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cdb881",
   "metadata": {},
   "source": [
    "### skimage에 있는 image와 text description으로 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "import skimage\n",
    "import IPython.display\n",
    "#import matplotlib.pyplot as plt\n",
    "#from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "# images in skimage to use and their textual descriptions\n",
    "descriptions = {\n",
    "    \"page\": \"a page of text about segmentation\",\n",
    "    \"chelsea\": \"a facial photo of a tabby cat\",\n",
    "    \"astronaut\": \"a portrait of an astronaut with the American flag\",\n",
    "    \"rocket\": \"a rocket standing on a launchpad\",\n",
    "    \"motorcycle_right\": \"a red motorcycle standing in a garage\",\n",
    "    \"camera\": \"a person looking at a camera on a tripod\",\n",
    "    \"horse\": \"a black-and-white silhouette of a horse\", \n",
    "    \"coffee\": \"a cup of coffee on a saucer\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image and text descriptions\n",
    "original_images = []\n",
    "texts = []\n",
    "plt.figure(figsize=(16, 5))\n",
    "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    if name not in descriptions:\n",
    "        continue\n",
    "\n",
    "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
    "    \n",
    "    plt.subplot(2, 4, len(original_images) + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"{filename}\\n{descriptions[name]}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    texts.append(descriptions[name])\n",
    "    original_images.append(image)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6384f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = [\"This is \" + desc for desc in texts]\n",
    "image_input = original_images\n",
    "inputs = processor(text=text_input, images=image_input, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "text_features = outputs.text_embeds.float()  # n_image x emb_dim [8, 512]\n",
    "image_features = outputs.image_embeds.float() # n_image x emb_dim [8, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features.shape, image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_features, text_features : almost unit vector\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True) # vector normalization to unit vector\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T # numpy matrix multiplications or dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654142dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(descriptions)\n",
    "\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.imshow(similarity, vmin=0.1, vmax=0.3)\n",
    "# plt.colorbar()\n",
    "plt.yticks(range(count), texts, fontsize=18)\n",
    "plt.xticks([])\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "for x in range(similarity.shape[1]):\n",
    "    for y in range(similarity.shape[0]):\n",
    "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "  plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "plt.xlim([-0.5, count - 0.5])\n",
    "plt.ylim([count + 0.5, -2])\n",
    "\n",
    "plt.title(\"Cosine similarity between text and image features\", size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6bd5c",
   "metadata": {},
   "source": [
    "### Zero-shot Image classification using cifar100 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100, CIFAR10\n",
    "\n",
    "cifar100 = CIFAR100(os.path.expanduser(\"~/.cache\"), download=True)\n",
    "dataset = cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02b73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = [f\"A photo of a {label}\" for label in dataset.classes]\n",
    "\n",
    "image_input = original_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=text_input, images=image_input, return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "text_features = outputs.text_embeds.float()\n",
    "image_features = outputs.image_embeds.float()\n",
    "text_features.shape, image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a77b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "top_probs, top_labels = text_probs.cpu().topk(5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "for i, image in enumerate(original_images):\n",
    "    plt.subplot(4, 4, 2 * i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(4, 4, 2 * i + 2)\n",
    "    y = np.arange(top_probs.shape[-1])\n",
    "    plt.grid()\n",
    "    plt.barh(y, top_probs[i])\n",
    "    #plt.barh(y, top_probs[i].detach().numpy())\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    #plt.yticks(y, [dataset.classes[index] for index in top_labels[i].numpy()])\n",
    "    plt.yticks(y, [text_input[index].lower().replace(\"a photo of \",\"\") for index in top_labels[i].numpy()])\n",
    "    plt.xlabel(\"probability\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e474549",
   "metadata": {},
   "source": [
    "### (ToDo) text 표현을 구분하는 표현이 부족해서 분류 성능이 떨어지는 점을 개선해 봅시다.\n",
    "\n",
    "classification 성능을 향상시킬 수 있도록 위 실험에서 text 표현을 추가해  보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcca2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input.extend([\"A photo of a written paper\", \"A photo of a cat\", \"a horse\", \"a horse icon\"])\n",
    "text_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab245a",
   "metadata": {},
   "source": [
    "### [참고] model.get_image_features(), get_text_features() 사용하여 구현할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b300cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "?model.get_image_features\n",
    "?model.get_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = original_images\n",
    "\n",
    "# extract image feature vectors\n",
    "image_inputs = processor(images=image_input, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_features = model.get_image_features(**image_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(hf_path + \"openai/clip-vit-base-patch32\")\n",
    "text_input = [f\"A photo of a {label}\" for label in dataset.classes]\n",
    "\n",
    "# extract text feature vectors\n",
    "text_inputs = tokenizer(text_input, padding=True, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    text_features = model.get_text_features(**text_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features.shape, image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312b361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_torch2i",
   "language": "python",
   "name": "venv_torch2i"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
